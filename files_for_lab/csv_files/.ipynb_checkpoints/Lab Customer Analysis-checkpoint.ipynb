{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c2f053a",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325b179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # keywords are highlighted in green, other strings in red, etc.\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#for plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#for regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler # do not use the function Normalise() - it does something entirely different\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "#categorical Variables\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb02f21",
   "metadata": {},
   "source": [
    "# Round 2 - Dealing with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe877c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_mca.to_csv('./marketing_customer_analysis.csv', index=False) #without indexing column!\n",
    "data_mca = pd.read_csv('./marketing_customer_analysis.csv')\n",
    "data_mca.shape\n",
    "\n",
    "data_mca = data_mca.drop(['Unnamed: 0'], axis=1)\n",
    "data_mca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5136ea3",
   "metadata": {},
   "source": [
    "### Round 2 - #1\n",
    "Show the dataframe shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d93135",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e71891",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mca.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1884dd2c",
   "metadata": {},
   "source": [
    "### Round 2 - #2\n",
    "Standardize header names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e993d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Headers without space\n",
    "data_mca = data_mca.rename(columns={'EmploymentStatus':'Employment Status'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eace4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Headers all on lower case\n",
    "mca = []\n",
    "for mcaname in data_mca.columns:\n",
    "    mca.append(mcaname.lower())\n",
    "data_mca.columns = mca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1692c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Headers replace the space by \"_\"\n",
    "data_mca.columns = data_mca.columns.str.replace(' ', '_')\n",
    "data_mca.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf36505",
   "metadata": {},
   "source": [
    "### Round 2 - #3 , #4\n",
    "Which columns are numerical? Which columns are categorical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c492297",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mca.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db14b7f1",
   "metadata": {},
   "source": [
    "### Round 2 - #5 \n",
    "Check and deal with NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bfbd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for duplicate rows in the data and remove if any.\n",
    "sum(data_mca.duplicated()) #check how many rows are duplicated.\n",
    "data_mca = data_mca.drop_duplicates()\n",
    "data_mca.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8369c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mca_percent = pd.DataFrame(round(data_mca.isna().sum()/len(data_mca),4)*100)\n",
    "# nulls_df = nulls_df.reset_index()\n",
    "# nulls_df.columns = ['header_name', 'percent_nulls']\n",
    "# nulls_df\n",
    "data_mca_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2f134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # strategy: drop rows that have null values (only if there are very few)\n",
    "data_mca[data_mca['state'].isna()==True].head(60)\n",
    "\n",
    "#We can see that 'state' and 'response' seem to be missing in the same rows. --> drop the rows.\n",
    "\n",
    "data_mca = data_mca[data_mca['state'].isna()==False]\n",
    "data_mca\n",
    "data_mca.isna().sum()\n",
    "data_mca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d83b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mca[data_mca['months_since_last_claim'].isna()==True].head(60)\n",
    "\n",
    "#We can see that 'months_since_last_claim' and 'number_of_open_complaints' seem to be missing in the same rows. --> drop the rows.\n",
    "data_mca = data_mca[data_mca['months_since_last_claim'].isna()==False]\n",
    "data_mca\n",
    "data_mca.isna().sum()\n",
    "#data_mca1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104686f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vehicle_class\n",
    "data_mca[data_mca['vehicle_class'].isna()==True].head(60)\n",
    "data_mca['vehicle_class'].unique()\n",
    "data_mca['vehicle_class'].mode()\n",
    "\n",
    "data_mca['vehicle_class'].value_counts(dropna=True)\n",
    "data_mca['vehicle_class'] = data_mca['vehicle_class'].fillna(data_mca['vehicle_class'].value_counts(dropna=True).index[0])\n",
    "\n",
    "data_mca.isna().sum()\n",
    "\n",
    "#merged_clean_ver1['state'] = merged_clean_ver1['state'].fillna(merged_clean_ver1['state'].value_counts(dropna=True).index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e9041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vehicle_size\n",
    "data_mca[data_mca['vehicle_size'].isna()==True].head(60)\n",
    "data_mca['vehicle_size'].unique()\n",
    "data_mca['vehicle_size'].mode()\n",
    "\n",
    "# --> fill it with the mean\n",
    "data_mca['vehicle_size'].value_counts(dropna=True)\n",
    "data_mca['vehicle_size'] = data_mca['vehicle_size'].fillna(data_mca['vehicle_size'].value_counts(dropna=True).index[0])\n",
    "\n",
    "data_mca.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90afb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vehicle_type --> nearly 50% is missing ! \n",
    "# strategy: input a value for the missing value (fill in a value that we choose or calculate)\n",
    "\n",
    "data_mca.describe(include=[np.object]).T #--> only 1 unique value and NaN!\n",
    "# data_mca[data_mca['vehicle_type'].isna()==True].head(60)\n",
    "data_mca['vehicle_type'].unique()\n",
    "\n",
    "# Assumption, that we have A = Automatic car and missing so \"not Automatic\", otherwise we could just drop the column.\n",
    "data_mca['vehicle_type'] = data_mca['vehicle_type'].fillna('not A') \n",
    "data_mca.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4dbccc",
   "metadata": {},
   "source": [
    "### Round 2 - #6\n",
    "Datetime format - Extract the months from the dataset and store in a separate column. \n",
    "Then filter the data to show only the information for the first quarter , \n",
    "ie. January, February and March. Hint: If data from March does not exist, consider only January and February."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7107ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88defb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mca.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4924f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#where is a date / month?\n",
    "data_mca.dtypes #no \"date\" there\n",
    "\n",
    "data_mca['effective_to_date'] = pd.to_datetime(data_mca['effective_to_date'], errors='coerce')\n",
    "data_mca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503aa2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mca['effective_to_date_month'] = data_mca['effective_to_date'].dt.month\n",
    "data_mca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652aceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mca['effective_to_date_month'].unique() # --> irritating, this means we have only information for Q1...\n",
    "\n",
    "data_mca_Q1 = data_mca[(data_mca['effective_to_date_month']==1) | (data_mca['effective_to_date_month']==2) | (data_mca['effective_to_date_month']==3)]\n",
    "data_mca_Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e89a0b",
   "metadata": {},
   "source": [
    "### Round 2 - #7\n",
    "#BONUS: Put all the previously mentioned data transformations into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a796c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define a function to clean the dataframe\n",
    "\n",
    "def clean_dataframe(x):\n",
    "    ## Standardize Headers\n",
    "    #Drop the first \"Unnamed: 0\" column\n",
    "    if 'Unnamed: 0' in x.columns:\n",
    "        x = x.drop(['Unnamed: 0'], axis=1)\n",
    "    else:\n",
    "        x = x\n",
    "    #Headers all on lower case and replace the space by \"_\"\n",
    "    header = []\n",
    "    for item in x.columns:\n",
    "        header.append(item.lower().replace(' ', '_'))\n",
    "    x.columns = header\n",
    "    #Headers without space\n",
    "    x = x.rename(columns={'EmploymentStatus':'Employment Status'})\n",
    "    \n",
    "    ## Check for duplicates\n",
    "    x = x.drop_duplicates()\n",
    "    \n",
    "    ## taking care of the NAN-Values:\n",
    "    #droping the NAN-Values for 'state' and 'response'\n",
    "    x = x[x['state'].isna()==False]\n",
    "    #droping the NAN-Values for 'months_since_last_claim' and 'number_of_open_complaints'\n",
    "    x = x[x['months_since_last_claim'].isna()==False]\n",
    "    #fill the 'vehicle_class' with the mean value\n",
    "    x['vehicle_class'] = x['vehicle_class'].fillna(x['vehicle_class'].value_counts(dropna=True).index[0])\n",
    "    #fill the 'vehicle_size' with the mean value\n",
    "    x['vehicle_size'] = x['vehicle_size'].fillna(x['vehicle_size'].value_counts(dropna=True).index[0])\n",
    "    #fill the NAN of vehicle_type with 'not A', because 50%\n",
    "    x['vehicle_type'] = x['vehicle_type'].fillna('not A')\n",
    "    #\n",
    "    x['effective_to_date'] = pd.to_datetime(x['effective_to_date'], errors='coerce')\n",
    "    x['effective_to_date_month'] = x['effective_to_date'].dt.month\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3667370",
   "metadata": {},
   "source": [
    "# Round 3 - EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f68757",
   "metadata": {},
   "source": [
    "###  Round 3 - #1 , #2\n",
    "Show DataFrame info, Describe Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b8727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describing the numerical columns\n",
    "summary_numerical = data_mca.describe().T \n",
    "summary_numerical\n",
    "\n",
    "#DataFrame info\n",
    "info_data = data_mca.info()\n",
    "info_data\n",
    "\n",
    "# additionnaly we add the range and the iqr\n",
    "summary_numerical['iqr'] = summary_numerical['75%']-summary_numerical['25%']\n",
    "\n",
    "summary_numerical['range'] = summary_numerical['max']-summary_numerical['min']\n",
    "summary_numerical\n",
    "\n",
    "#round up the values to 2 decimals (round() would round to 0 decimals) --> with function\n",
    "def roundforme(x):\n",
    "    return round(x,2)\n",
    "\n",
    "#to round the whole table..\n",
    "for col in summary_numerical.columns:\n",
    "    summary_numerical[col] = summary_numerical[col].apply(roundforme)\n",
    "summary_numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b800693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describing the object columns\n",
    "summary_objects = data_mca.describe(include=[np.object]).T\n",
    "summary_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5403bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08774baf",
   "metadata": {},
   "source": [
    "###  Round 3 - #3\n",
    "Show a plot of the total number of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd6312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=data_mca['response'])\n",
    "plt.ylabel('Total number of responses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b49226",
   "metadata": {},
   "source": [
    "###  Round 3 - #4\n",
    "Show a plot of the response rate by the sales channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1ecc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data_mca['numerical_response'] = data_mca['response'].map(dict(Yes=1,No=0))*100 #in %!\n",
    "data_mca\n",
    "\n",
    "sns.barplot(x=\"sales_channel\", y=\"numerical_response\", data=data_mca)\n",
    "\n",
    "plt.xlabel(\"Sales Channel\")\n",
    "plt.ylabel(\"Numerical Response in [%]\")\n",
    "plt.title(\"Response rate by Sales Channel\") # You can comment this line out if you don't need title\n",
    "#axes.set(ylim=(0, 100))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6422c303",
   "metadata": {},
   "source": [
    "###  Round 3 - #5\n",
    "Show a plot of the response rate by the total claim amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186ab10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform data binning on points variable with specific quantiles and labels\n",
    "data_mca['claims_bin'] = pd.qcut(data_mca['total_claim_amount'], q=10)\n",
    "data_mca\n",
    "\n",
    "sns.barplot(x = data_mca['total_claim_amount'],y=data_mca['claims_bin'] ,data=data_mca)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2716e74",
   "metadata": {},
   "source": [
    "###  Round 3 - #6\n",
    "Show a plot of the response rate by income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028e75d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mca['income_bin'] = pd.qcut(data_mca['income'], q=10, duplicates='drop')\n",
    "# data_mca['income_bin'] = numpy.round_(data_mca['income_bin'], decimals = 0, out = None)\n",
    "# data_mca['income_bin'] = [ round(elem, 2) for elem in data_mca['income_bin'] ]\n",
    "data_mca\n",
    "\n",
    "sns.barplot(x = data_mca['income'],y=data_mca['income_bin'] ,data=data_mca)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7870996",
   "metadata": {},
   "source": [
    "# Round 4 - Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4424a42",
   "metadata": {},
   "source": [
    "###  Round 4 - #1\n",
    "Check the data types of the columns. Get the numeric data into dataframe called numerical and categorical columns in a dataframe called categoricals. (You can use np.number and np.object to select the numerical data types and categorical data types respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dee7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data types\n",
    "data_mca.dtypes\n",
    "\n",
    "categoricals = data_mca.select_dtypes(include=np.object)\n",
    "categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e615d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = data_mca.select_dtypes(include=np.number)\n",
    "numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fb2f84",
   "metadata": {},
   "source": [
    "###  Round 4 - #2\n",
    "Now we will try to check the normality of the numerical variables visually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4fbf09",
   "metadata": {},
   "source": [
    "#### Round 4 - #2.1\n",
    "Use seaborn library to construct distribution plots for the numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9060ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab4e525",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in numerical.columns:\n",
    "    sns.distplot(numerical,x=numerical[i])\n",
    "    plt.xlabel(i)\n",
    "    plt.ylabel('Distribution')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a65ca8",
   "metadata": {},
   "source": [
    "#### Round 4 - #2.2\n",
    "Use Matplotlib to construct histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f111677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://matplotlib.org/stable/gallery/pyplots/pyplot_text.html#sphx-glr-gallery-pyplots-pyplot-text-py\n",
    "\n",
    "for i in numerical.columns:\n",
    "    plt.hist(numerical[i], bins=20)\n",
    "    plt.xlabel(i)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e352c55a",
   "metadata": {},
   "source": [
    "#### Round 4 - #2.3\n",
    "Do the distributions for different numerical variables do look like a normal distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6995a06b",
   "metadata": {},
   "source": [
    "The Distribution of the Customer Lifetime Value looks normally distributed. \n",
    "Total Claim Amount looks somewhat normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538de994",
   "metadata": {},
   "source": [
    "#### Round 4 - #3 - #4\n",
    "For the numerical variables, check the multicollinearity between the features. Please note that we will use the column total_claim_amount later as the target variable.\n",
    "\n",
    "Drop one of the two features that show a high correlation between them (greater than 0.9). Write code for both the correlation matrix and for seaborn heatmap. If there is no pair of features that have a high correlation, then do not drop any features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64accd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_matrix = numerical.corr()\n",
    "correlations_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bb3847",
   "metadata": {},
   "outputs": [],
   "source": [
    "#heatmap\n",
    "sns.heatmap(correlations_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7220d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.9 and 1.0 very highly correlated\n",
    "# 0.7 and 0.9 highly correlated\n",
    "# 0.5 and 0.7 moderately correlated\n",
    "# 0.3 and 0.5 low correlation\n",
    "# less than 0.3 little if any (linear) correlation. \n",
    "\n",
    "# Target Value: Total Claim Amount \n",
    "# is moderately correlated with monthly_premium_auto (0.63)\n",
    "# is lowly correlated with income (-0.35)\n",
    "\n",
    "# There is no correlation between monthly premium auto and income (-0.0014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8439b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced_numerical = numerical[['total_claim_amount', 'income', 'monthly_premium_auto']]\n",
    "# reduced_numerical.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79251640",
   "metadata": {},
   "source": [
    "# Round 5 - Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbfc5a5",
   "metadata": {},
   "source": [
    "#### Round 5 - #1 X-y split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aed3020",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_mca['total_claim_amount']\n",
    "X = data_mca.drop(['total_claim_amount'], axis=1)\n",
    "X.head()\n",
    "X.shape\n",
    "display(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45554cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping useless columns\n",
    "X = X.drop(['effective_to_date_month', 'numerical_response', 'customer','effective_to_date','income_bin','claims_bin'],axis=1)\n",
    "X.head()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5cc00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b72ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = X.select_dtypes(np.number)\n",
    "X_cat = X.select_dtypes(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b15297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0505dec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f859756",
   "metadata": {},
   "source": [
    "#### Round 5 - #2 Normalize (numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07607fc1",
   "metadata": {},
   "source": [
    "Normalization or Min-Max Scaling is used to transform features to be on a similar scale. \n",
    "\n",
    "The new point is calculated as:\n",
    "X_new = (X - X_min)/(X_max - X_min)\n",
    "\n",
    "This scales the range to [0, 1] or sometimes [-1, 1]. \n",
    "Geometrically speaking, transformation squishes the n-dimensional data into an n-dimensional \n",
    "unit hypercube. Normalization is useful when there are no outliers as it cannot cope up with \n",
    "them. Usually, we would scale age and not incomes because only a few people have high incomes \n",
    "but the age is close to uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7e405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code for the MIN-MAX-Normalizing, in this lab I chose to do the Standard Normalizing, \n",
    "# because it takes into account more the outliers.\n",
    "\n",
    "# Normalizing data: make data range from 0 - 1, instead of from min to max\n",
    "transformer = MinMaxScaler().fit(X_num)\n",
    "X_num_normalized = transformer.transform(X_num)\n",
    "print(X_num_normalized.shape)\n",
    "\n",
    "X_num_normalized=pd.DataFrame(X_num_normalized, columns=X_num.columns)\n",
    "X_num_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e62b2",
   "metadata": {},
   "source": [
    "Standardization or Z-Score Normalization is the transformation of features \n",
    "by subtracting from mean and dividing by standard deviation. This is often called as Z-score.\n",
    "\n",
    "X_new = (X - mean)/Std\n",
    "\n",
    "Standardization can be helpful in cases where the data follows a Gaussian distribution. \n",
    "However, this does not have to be necessarily true. Geometrically speaking, it translates \n",
    "the data to the mean vector of original data to the origin and squishes or expands the \n",
    "points if std is 1 respectively. We can see that we are just changing mean and standard \n",
    "deviation to a standard normal distribution which is still normal thus the shape of the \n",
    "distribution is not affected.\n",
    "\n",
    "Standardization does not get affected by outliers because there is no predefined range of transformed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625d17ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Standardization or Z-Score Normalization\n",
    "# transformer = StandardScaler().fit(X_num)\n",
    "# X_num_normalized = transformer.transform(X_num)\n",
    "# print(X_num_normalized.shape)\n",
    "\n",
    "# X_num_normalized= pd.DataFrame(X_num_normalized, columns=X_num.columns)\n",
    "# X_num_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891de745",
   "metadata": {},
   "source": [
    "# Round 6 - Processing Data, Linear Regression, Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e30d5e1",
   "metadata": {},
   "source": [
    "#### Round 6 - Processing Data - #1 One Hot/Label Encoding (categorical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f176bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864eab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding is a way to turn categorical variables into multiple numerical columns\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(drop='first').fit(X_cat) # the first one is the most frequent one.\n",
    "#print(encoder.categories_)\n",
    "encoded = encoder.transform(X_cat).toarray()\n",
    "#print(encoded)\n",
    "\n",
    "cols = encoder.get_feature_names(input_features=X_cat.columns)\n",
    "cols\n",
    "\n",
    "onehot_encoded = pd.DataFrame(encoded, columns=cols)\n",
    "onehot_encoded.head()\n",
    "onehot_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98135c66",
   "metadata": {},
   "source": [
    "#### Round 6 - Processing Data - #2 Concat DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1185bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_num_normalized, onehot_encoded], axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeadf23",
   "metadata": {},
   "source": [
    "#### Round 6 - Linear Regression - #3 Train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c6f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reset_index(drop=True)\n",
    "#onehot_encoded=onehot_encoded.reset_index(drop=True)\n",
    "X = X.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076b2877",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# 0.9 and 1.0 very highly correlated\n",
    "# 0.7 and 0.9 highly correlated\n",
    "# 0.5 and 0.7 moderately correlated\n",
    "# 0.3 and 0.5 low correlation\n",
    "# less than 0.3 little if any (linear) correlation. \n",
    "###\n",
    "\n",
    "### Now we do the correlation for the categoricals:\n",
    "\n",
    "Collinearity = pd.concat((y, X), axis=1)\n",
    "Collinearity\n",
    "\n",
    "corr_matrix_cat = Collinearity.corr()\n",
    "corr_matrix_cat #44x44-matrix!\n",
    "#print(corr_matrix_cat.iloc[0])\n",
    "\n",
    "filtered_matrix = corr_matrix_cat[((corr_matrix_cat >= .3) | (corr_matrix_cat <= -.3))]\n",
    "filtered_matrix\n",
    "print(filtered_matrix.iloc[0])\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "sns.heatmap(filtered_matrix, annot=True, cmap=\"Reds\")\n",
    "plt.show()\n",
    "\n",
    "### From numericals:\n",
    "# Target Value: Total Claim Amount \n",
    "# is moderately correlated with monthly_premium_auto (0.63)\n",
    "# is lowly correlated with income (-0.35)\n",
    "# There is no correlation between monthly premium auto and income (-0.0014)\n",
    "\n",
    "### Result from categoricals:\n",
    "# Target Value: Total Claim Amount\n",
    "# is lowly correlated with employment_status_Employed (-0.33)\n",
    "# is lowly correlated with employment_status_Unemployed (-0.32)\n",
    "# is moderately correlated with location_code_Suburban (0.57)\n",
    "# is lowly correlated with vehicle_class_Luxury Car (0.31)\n",
    "# is lowly correlated with vehicle_class_Luxury SUV (0.31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d04ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## From Round 7: use the concept of multicollinearity and remove insignificant variables\n",
    "# so check if there is a correlation between the 7 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ec8e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_matrix['monthly_premium_auto']\n",
    "\n",
    "#lowly correlated with \"vehicle_class_Luxury Car\" (0.451538)\n",
    "#lowly correlated with \"vehicle_class_Luxury SUV\" (0.484589)\n",
    "\n",
    "#makes sense... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef85d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_matrix['income']\n",
    "\n",
    "#highly correlated with \"employment_status_Employed\" (0.794671)\n",
    "#highly correlated with \"employment_status_Unemployed\" (-0.722468)\n",
    "#lowly correlated with \"location_code_Suburban\" (-0.450730)\n",
    "\n",
    "#--> toss the employment status Employed and Unemployed and keep only income!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8442e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_matrix['location_code_Suburban']\n",
    "#lowly correlated with \"income\" (-0.450730)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c44f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_matrix['vehicle_class_Luxury Car']\n",
    "#lowly correlated with \"monthly_premium_auto\" (0.451538)\n",
    "\n",
    "#--> toss the vehicle_class Luxury Car/SUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4067b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "y # ['total_claim_amount'] from above.  \n",
    "X = X[['monthly_premium_auto', 'income', 'employment_status_Employed','employment_status_Unemployed','location_code_Suburban','vehicle_class_Luxury Car', 'vehicle_class_Luxury SUV']]\n",
    "\n",
    "# #Round 7: try without \"insignificant variables\" employment_status, vehicle_class Luxury Car/SUV\n",
    "# X = X[['monthly_premium_auto','location_code_Suburban']]\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d3bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do the Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We caculate the linear regression based on the \"train\"-data\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(X_train,y_train)\n",
    "\n",
    "print(\"b0 = \" , lm.intercept_)\n",
    "print(\"b1 = \" , lm.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb45b2d5",
   "metadata": {},
   "source": [
    "#### Round 6 - Linear Regression -  #4 Apply linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a36d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compare the predicted y-data (applied linear regression on x_train) and compare it with our y_train data.\n",
    "predictions = lm.predict(X_train)\n",
    "r2_score(y_train, predictions)\n",
    "# our r2 score is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52919f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f3dea64",
   "metadata": {},
   "source": [
    "#### Round 6 - Model Validation -  #5 Description R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ce461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply the linear regression on the x-test and compare it with our y-test\n",
    "predictions_test = lm.predict(X_test)\n",
    "R2 = r2_score(y_test, predictions_test)\n",
    "print(\"R2-score is \", R2)\n",
    "# our r2 score is even better, why ? This shouldn't be the case.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f073017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb96c64",
   "metadata": {},
   "source": [
    "#### Round 6 - Model Validation -  #6 Description MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ae9ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse=mean_squared_error(y_test,predictions_test)\n",
    "mse\n",
    "print(\"mean squared error (MSE) is \", mse)\n",
    "\n",
    "# --> the mean squared error is the error ^2 so the error seems extremely large! that is why to have a \n",
    "# better understanding/relation with the data, the RMSE is more \"relatable\"\n",
    "\n",
    "#Round 6: 21780.55771867215\n",
    "#Round 7 without Employment Status: 22531.810977831443\n",
    "#Round 7 without Employment Status + Luxury Car/SUV: 22641.020956354456\n",
    "#Round 7 only 'monthly_premium_auto','location_code_Suburban': 23960.230603560052\n",
    "#Round 7 with MIN-MAX-Scale: 21780.55771867215 --> no difference.. ? why ?\n",
    "#Round 7 with 0.15: 20785.98606442319 !!! Better !!!\n",
    "#Round 7 with 0.25: 22890.035134861595, worse...\n",
    "#Round 7 with 0.10: 21348.528873555086 !!! Better !!!\n",
    "#Round 7 with 0.05: 20300.42282084946 !!! Better !!!\n",
    "\n",
    "# --> by removing some variables, the error couldn't be made better..\n",
    "# --> we keep all the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09300883",
   "metadata": {},
   "source": [
    "#### Round 6 - Model Validation -  #7 Description RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6171263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test,predictions_test))\n",
    "rmse\n",
    "print(\"root mean squared error (RMSE) is \", rmse)\n",
    "\n",
    "# --> the RMSE is compared to the five couples (y_test[:5] and predictions_test[:5]) the triple than expected..\n",
    "\n",
    "#Round 6: 147.58237604359184\n",
    "#Round 7 without Employment Status: 150.10599914004584 --> not very helpful...\n",
    "#Round 7 without Employment Status + Luxury Car/SUV: 150.4693356014921\n",
    "#Round 7 only 'monthly_premium_auto','location_code_Suburban': 154.7909254561134\n",
    "#Round 7 with MIN-MAX-Scale: 147.58237604359184 --> no difference.. ? why ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd58697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Have a look at another sample, to get an impression of the error...\n",
    "abs(y_test[50:60]-predictions_test[50:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71cbd95",
   "metadata": {},
   "source": [
    "#### Round 6 - Model Validation -  #8 Description MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d5b892",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, predictions_test)\n",
    "print(\"mean absolute error (MAE) is \", mae)\n",
    "\n",
    "#Round 6: 109.21649592507937\n",
    "#Round 7 without Employment Status: 113.20024356020065\n",
    "#Round 7 without Employment Status + Luxury Car/SUV: 113.62662388135328\n",
    "#Round 7 only 'monthly_premium_auto','location_code_Suburban': 118.55317357733347\n",
    "#Round 7 with MIN-MAX-Scale: 109.21649592507936 --> no difference.. ? why ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea92eba",
   "metadata": {},
   "source": [
    "# Round 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b2c376",
   "metadata": {},
   "source": [
    "Build a function, from round 2 and round 7, to clean and process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4c2d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define a function to clean the dataframe\n",
    "\n",
    "def clean_and_process(x):\n",
    "    ## Round 2\n",
    "    ## Standardize Headers\n",
    "    #Drop the first \"Unnamed: 0\" column\n",
    "    if 'Unnamed: 0' in x.columns:\n",
    "        x = x.drop(['Unnamed: 0'], axis=1)\n",
    "    else:\n",
    "        x = x\n",
    "    #Headers all on lower case and replace the space by \"_\"\n",
    "    header = []\n",
    "    for item in x.columns:\n",
    "        header.append(item.lower().replace(' ', '_'))\n",
    "    x.columns = header\n",
    "    #Headers without space\n",
    "    x = x.rename(columns={'EmploymentStatus':'Employment Status'})\n",
    "    \n",
    "    ## Check for duplicates\n",
    "    x = x.drop_duplicates()\n",
    "    \n",
    "    ## taking care of the NAN-Values:\n",
    "    #droping the NAN-Values for 'state' and 'response'\n",
    "    x = x[x['state'].isna()==False]\n",
    "    #droping the NAN-Values for 'months_since_last_claim' and 'number_of_open_complaints'\n",
    "    x = x[x['months_since_last_claim'].isna()==False]\n",
    "    #fill the 'vehicle_class' with the mean value\n",
    "    x['vehicle_class'] = x['vehicle_class'].fillna(x['vehicle_class'].value_counts(dropna=True).index[0])\n",
    "    #fill the 'vehicle_size' with the mean value\n",
    "    x['vehicle_size'] = x['vehicle_size'].fillna(x['vehicle_size'].value_counts(dropna=True).index[0])\n",
    "    #fill the NAN of vehicle_type with 'not A', because 50%\n",
    "    x['vehicle_type'] = x['vehicle_type'].fillna('not A')\n",
    "    #\n",
    "    x['effective_to_date'] = pd.to_datetime(x['effective_to_date'], errors='coerce')\n",
    "    x['effective_to_date_month'] = x['effective_to_date'].dt.month\n",
    "    \n",
    "    ## Round 3\n",
    "    return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
